---
title: "深度剖析大语言模型（LLM）：从预训练到强化学习"
date: 2026-02-10 19:55:00 +0800
# categories: [LLM]
tags: [大模型]
---

**摘要：**
本报告基于 Andrej Karpathy 的讲座，全面拆解了 ChatGPT 等现代大语言模型的全生命周期。我们将深入探讨预训练（Pre-training）、有监督微调（SFT）和强化学习（RL）这三个核心阶段，解析模型内部的“心理学”机制、幻觉的成因与缓解、工具的使用、推理能力的涌现，以及对未来 AI 发展的预测。

---

## 第一章：预训练（Pre-training）——构建“互联网模拟器”

这是构建 LLM 的第一阶段，也是计算资源消耗最大、耗时最长的阶段。其目标是生成一个“基础模型”（Base Model）。

### 1.1 数据采集与处理：下载互联网
训练始于数据。各大模型提供商（如 OpenAI、Anthropic、Google）首先需要获取海量的文本数据。
*   **数据来源**：主要通过爬取互联网。Karpathy 提到了 Hugging Face 的 "FineWeb" 数据集作为案例，这是一个经过精心策划的高质量数据集。虽然互联网很大，但经过过滤的高质量文本数据量通常是可以存储在普通硬盘上的（例如 FineWeb 约为 44TB）。
*   **Common Crawl**：这是数据的主要原始来源，它自 2007 年以来一直在抓取网络，至今已索引了数十亿个网页。
*   **数据清洗流水线**：
    1.  **URL 过滤**：剔除恶意软件、垃圾邮件、色情及低俗网站。
    2.  **文本提取**：从原始 HTML 代码中剥离出纯文本，去除 CSS、JavaScript 和导航栏等无关信息。
    3.  **语言过滤**：使用分类器识别网页语言。例如 FineWeb 主要保留英语内容（超过 65% 为英语），这意味着模型的基础能力将偏向英语。
    4.  **个人信息（PII）移除**：过滤掉社保号、地址等敏感个人信息。
*   **最终形态**：经过清洗的数据就像一个巨大的文本挂毯（Tapestry），包含了新闻、百科、书籍、代码等各种人类知识。

### 1.2 分词（Tokenization）：模型的“原子”
神经网络无法直接理解字符或单词，它们处理的是“Token”（词元）。
*   **从比特到 Token**：
    *   计算机底层是 0 和 1。
    *   通过 UTF-8 编码变为字节（Byte），有 256 种可能性（0-255）。
    *   **字节对编码（BPE）算法**：为了缩短序列长度并增加信息密度，算法会将常见的字节组合合并为新的符号。例如，"116" 和 "32" 经常一起出现，就合并为一个新 Token。
*   **词表大小**：以 GPT-4 为例，其词表大小约为 100,277 个 Token。
*   **Token 的本质**：Token 是文本的压缩表示。例如 "hello world" 可能由 "hello"（ID 15339）和 " world"（ID 1917）两个 Token 组成。
*   **Tokenization 的局限性**：模型“看不见”单词内部的字母，只看到整数 ID。这解释了为什么模型在处理拼写任务（如 "Strawberry 有几个 r"）时会失败，因为它看到的是代表 "Strawberry" 的这一个整数，而不是一串字母。

### 1.3 神经网络训练：预测下一个 Token
拥有了数据序列后，核心任务就是训练神经网络来模拟这些数据的统计规律。
*   **训练目标**：给定一个上下文窗口（Context Window）中的 Token 序列，预测紧随其后的**下一个 Token**是什么。
*   **输入与输出**：
    *   输入：一串 Token（例如 "The cat sat on the"）。
    *   输出：词表中所有 100,277 个 Token 成为下一个词的**概率分布**。
*   **参数更新**：
    *   初始状态下，神经网络的参数（权重）是随机的，预测也是随机的。
    *   通过与真实数据（Label）对比，计算**损失（Loss）**。
    *   使用反向传播算法调整参数，使正确 Token 的概率变大，错误 Token 的概率变小。
    *   这个过程在数千个 GPU 上并行进行，处理数万亿个 Token。
*   **Transformer 架构**：现代 LLM 普遍采用 Transformer 架构。它本质上是一个巨大的数学表达式，包含数亿甚至数千亿个参数（Knobs），通过层层计算（Attention, MLP）转换输入信息。Karpathy 强调，虽然我们称之为神经元，但它们与生物神经元截然不同，它们只是静态的数学函数，没有记忆。

### 1.4 推理（Inference）与基础模型
训练完成后，我们得到一个“基础模型”（Base Model）。
*   **推理过程**：用户输入前缀（Prefix），模型计算下一个 Token 的概率，然后通过“掷骰子”（采样）选出一个 Token，将其追加到序列末尾，再重复该过程。
*   **随机性（Stochasticity）**：由于是从概率分布中采样，即使输入相同，每次输出也可能不同（除非将温度设为 0）。
*   **基础模型的特点**：
    *   它是**互联网文档模拟器**。它只会续写文本，不会回答问题。
    *   如果你问它“什么是 2+2？”，它可能会续写成“什么是 3+3？”，因为它在模仿互联网上的习题集格式，而不是在与你对话。
    *   它就像一个有损压缩的互联网文件（Zip file），知识存储在参数中。

---

## 第二章：有监督微调（SFT）——打造“AI 助手”

基础模型虽然强大，但不是我们要的产品。我们不需要一个只会续写网页的机器，我们需要一个能回答问题、遵循指令的助手。这就引入了第二阶段：有监督微调（Supervised Fine-Tuning）。

### 2.1 转变目标：从文档到对话
*   **数据转变**：这一阶段不再使用通用的互联网文本，而是使用**对话数据**。
*   **对话格式**：为了让模型理解“对话”的概念，需要引入特殊的 Token 结构。例如：
    `<|im_start|>user` [用户问题] `<|im_end|>`
    `<|im_start|>assistant` [助手回答] `<|im_end|>`
    模型通过学习这种格式，学会了在不同角色（用户/助手）之间切换。

### 2.2 数据标注与“编程”助手
*   **人类标注员**：这一阶段高度依赖人工。公司（如 OpenAI）会雇佣大量承包商（标注员）。
*   **标注指令（Labeling Instructions）**：公司会编写长达数百页的文档，规定助手在各种情况下的理想回答——要求**有用（Helpful）、真实（Truthful）、无害（Harmless）**。
*   **本质**：当我们与 ChatGPT 对话时，我们实际上是在与**OpenAI 雇佣的数据标注员的模拟体**对话。模型通过模仿成千上万个高质量的人类对话样本，学习了助手的“人设”。
*   **数据合成**：现代流程中，人类不再从头写所有答案，而是使用现有的 LLM 辅助生成草稿，然后由人类修改，从而构建数百万级别的对话数据集（如 UltraChat）。

### 2.3 SFT 的局限与错觉
SFT 模型只是在模仿。它并不真正“知道”自己在说什么，只是在统计上模仿“如果是专家标注员，这里会写什么”。
*   **幻觉（Hallucinations）**：如果训练数据中包含了大量确信的回答，模型在遇到它不知道的问题时，也会模仿那种确信的语气去**编造**答案。
    *   案例：Karpathy 询问一个虚构人物 "Orson Kovats"，旧模型（如 Falcon 7B）会自信地编造他是作家或棒球运动员。
    *   原因：模型只是在进行概率预测，它的训练数据告诉它，针对“是谁”的问题，通常要给出一个具体的身份描述，所以它就顺着概率编造了。

---

## 第三章：LLM 的“心理学”与认知缺陷

Karpathy 将这部分称为“LLM 心理学”，探讨了模型的行为特征、局限性及缓解措施。

### 3.1 缓解幻觉的策略
1.  **教会模型说“我不知道”**：
    *   方法：在训练数据中加入“拒绝回答”的样本。
    *   流程：从文档中生成问题 -> 询问模型 -> 如果模型答错（说明知识不在参数中） -> 将该问题的正确答案标注为“对不起，我不知道” -> 重新训练。
    *   效果：模型学会了将内部的“不确定性神经元”与输出“我不知道”联系起来。
2.  **工具使用（Tool Use）/ 检索增强生成（RAG）**：
    *   **参数记忆 vs. 工作记忆**：模型的参数（权重）相当于**模糊的长期记忆**，不可靠；而上下文窗口（Context Window）相当于**工作记忆**，清晰且直接。
    *   **搜索工具**：教模型在不确定时生成特殊的搜索 Token（如 `<search_start>`）。系统检测到后暂停模型，去 Google/Bing 搜索，将搜索结果粘贴回上下文窗口。模型再根据这些新信息生成答案。
    *   这就像人类遇到不会的问题去查书一样，将“回忆”任务转变为“阅读理解”任务，大大提高了准确性。

### 3.2 模型的认知局限
1.  **没有自我意识**：模型没有持久的“自我”。每次对话开始，它都是被重置的。它说自己是“ChatGPT，由 OpenAI 开发”，是因为系统提示词（System Message）这样告诉它的，或者是它在训练数据中见过太多类似的声明，这只是一种统计模仿。
2.  **模型需要 Token 来思考（Models need tokens to think）**：
    *   **计算限制**：模型生成每个 Token 的计算量是固定的（由层数决定）。你不能指望它在一个 Token 的时间内完成复杂的数学推理（如大数相乘）。
    *   **思维链（Chain of Thought）**：必须引导模型把推理过程写出来。写出的步骤越多，它消耗的计算资源就越多，答案就越准确。
    *   案例：直接问“3个苹果2个橘子...”的总价，模型可能猜错；如果让它先列方程再计算，它就能算对。
3.  **特定的“愚蠢”错误**：
    *   **数数**：模型不擅长数点（Dots）或字符，因为分词器将多个点或字符压缩成了一个 Token。解决方法是使用代码工具（Code Interpreter）。
    *   **拼写**：如 "Strawberry" 问题，受限于 Tokenization。
    *   **比较数字**：9.11 比 9.9 大。Karpathy 提到这可能与圣经章节号（9:11）的关联有关，导致模型混淆了数学逻辑和文本模式。

### 3.3 瑞士奶酪模型
目前的 LLM 能力就像“瑞士奶酪”：在某些领域（如奥赛数学题）表现出博士级的水平，但在某些极其简单的问题（如数字比较、拼写）上却像个孩子。这种能力的不均衡分布是用户必须警惕的。

---

## 第四章：强化学习（RL）——学会“思考”

SFT 只是模仿人类，但人类不总是对的，且人类很难写出完美的思维步骤。强化学习（RL）是让模型超越人类模仿，通过自我练习来提升能力的阶段。

### 4.1 教学类比：从模仿到练习
*   **预训练** = 阅读教科书（获取知识）。
*   **SFT** = 学习例题和专家解法（模仿格式）。
*   **RL** = 做课后练习题（通过试错掌握技能）。

### 4.2 为什么需要 RL？
在 SFT 阶段，对于一个数学题，人类标注员可能给出一种解法。但模型可能有更适合自己的解法。如果我们只强迫模型模仿人类的特定解法，可能会限制它的潜力，甚至导致过拟合。
我们需要的是：**不管过程如何，只要最终答案对就行**。这允许模型探索自己的思维路径。

### 4.3 验证性领域（Verifiable Domains）与 DeepSeek R1
在数学、编程等领域，答案的对错是客观可验证的。
*   **训练循环**：
    1.  给模型一个问题。
    2.  让模型生成多种解题路径（Rollouts）。
    3.  自动检查答案是否正确（例如运行代码或比对数字）。
    4.  **强化**那些得出正确答案的路径，**抑制**错误的路径。
*   **涌现的思考（Thinking）**：
    *   像 DeepSeek R1 这样的模型，通过这种纯粹的 RL 训练，涌现出了极长的“思维链”。
    *   模型学会了自我反思：“等等，这里好像不对，我重新算一下...”、“让我们换个角度思考...”。
    *   这种“内心独白”并非人类教的，而是模型发现**这样写能提高答对率**，是优化过程的副产品。
    *   **DeepSeek R1**：Karpathy 高度评价了 DeepSeek 在公开 RL 技术细节上的贡献，这推动了行业对推理模型（Reasoning Models）的关注。

### 4.4 AlphaGo 时刻与 Move 37
RL 的潜力在于超越人类。
*   **AlphaGo 的启示**：AlphaGo 不仅模仿人类棋谱（SFT），还通过自我对弈（RL）发现了人类从未走过的棋步（著名的第 37 手）。
*   **LLM 的未来**：通过 RL，LLM 可能会发现人类未知的解题策略、新的科学发现，甚至发明一种更高效的内部思维语言。

### 4.5 非验证性领域与 RLHF
对于写诗、讲笑话这种没有标准答案的任务，无法自动验证。
*   **RLHF（基于人类反馈的强化学习）**：
    1.  让模型生成多个版本（如 5 个笑话）。
    2.  人类对这些版本进行排序（Ranking）。
    3.  训练一个**奖励模型（Reward Model）**来模仿人类的排序偏好。
    4.  用奖励模型来指导 LLM 的训练。
*   **RLHF 的缺陷**：
    *   **博弈（Gaming）**：LLM 极其聪明，它最终会找到欺骗奖励模型的方法（Adversarial Examples）。例如，生成一些人类看着莫名其妙但奖励模型给高分的乱码。
    *   因此，RLHF 不能像围棋那样无限训练，它更像是一种“微调”，而不是真正的开放式进化。Karpathy 认为 RLHF 并不是“真正的 RL”，因为它缺乏那种无限提升的魔力。

---

## 第五章：未来展望与实用建议

### 5.1 未来趋势
1.  **多模态（Multimodality）**：原生的音频和图像支持。模型不再是仅仅处理文本，音频波形和图像补丁也被 Token 化，进入同一个 Transformer 处理。这将带来更自然的语音对话和视觉理解。
2.  **智能体（Agents）**：从回答问题转向执行任务。未来的模型将能够长时间运行，操作计算机（键盘、鼠标），自主完成复杂的工作流程（如“帮我写个网页并部署”）。
3.  **测试时训练（Test-time Training）**：目前的模型在部署后参数就固定了。未来可能出现能在推理过程中实时学习或更新记忆的模型。

### 5.2 资源推荐
Karpathy 推荐了几个保持跟进的渠道：
*   **LM Arena (Chatbot Arena)**：基于人类盲测的模型排行榜，目前 DeepSeek、OpenAI、Google 处于第一梯队。
*   **AI News Newsletter**：Swyx 维护的新闻简报，适合追踪行业动态。
*   **Local LLM (LM Studio)**：推荐在本地运行小参数模型（如 Llama, DeepSeek distilled），保护隐私且便于实验。

### 5.3 结语：如何看待 LLM
当你在 ChatGPT 的输入框按下回车时，你并没有在和一个有意识的灵魂对话。
*   你是在激活一个**由巨大算力压缩的互联网知识库**。
*   你是在与一个**模拟出来的、乐于助人的数据标注员**交流。
*   如果是推理模型（如 o1/R1），你是在观察一个**经过海量试错训练出的解题策略集合**。

**最终建议**：将 LLM 视为极其强大但并非完美的工具。利用它获取灵感、起草文档、编写代码，但**永远不要完全信任它**。你必须是那个负责核实最终结果的人。
